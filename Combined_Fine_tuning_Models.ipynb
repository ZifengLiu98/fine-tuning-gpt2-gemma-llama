{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0066daec",
   "metadata": {},
   "source": [
    "## Fine-tuning Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e600fa8",
   "metadata": {},
   "source": [
    "## Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54221306",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets peft bitsandbytes accelerate openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e058c9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.11/site-packages (4.48.0)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in /opt/homebrew/lib/python3.11/site-packages (1.2.1)\n",
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/liuzifeng/Library/Python/3.11/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.3)\n",
      "Requirement already satisfied: psutil in /Users/liuzifeng/Library/Python/3.11/lib/python/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/homebrew/lib/python3.11/site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (from bitsandbytes) (1.11.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/liuzifeng/Library/Python/3.11/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/liuzifeng/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes, peft\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [peft][32m1/2\u001b[0m [peft]\n",
      "\u001b[1A\u001b[2KSuccessfully installed bitsandbytes-0.42.0 peft-0.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a42cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6636/6636 [00:00<00:00, 9404.67 examples/s]\n",
      "Map: 100%|██████████| 1660/1660 [00:00<00:00, 10681.08 examples/s]\n",
      "Downloading shards:   0%|          | 0/4 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 63\u001b[0m\n\u001b[1;32m     46\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(tokenize, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# ========= Step 5: 准备 QLoRA 模型 =========\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# bnb_config = BitsAndBytesConfig(\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#     load_in_4bit=True,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#     quantization_config=bnb_config\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m model \u001b[38;5;241m=\u001b[39m prepare_model_for_kbit_training(model)\n\u001b[1;32m     71\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     72\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     73\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/modeling_utils.py:3944\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3941\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3943\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3944\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3953\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3955\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3956\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3960\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3962\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3963\u001b[0m ):\n\u001b[1;32m   3964\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/utils/hub.py:1098\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1009\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1007\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1009\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1020\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1543\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1541\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1543\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1553\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    450\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[1;32m    454\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:934\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 934\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    937\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:877\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 877\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:812\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    809\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 812\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/response.py:797\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ========= Step 1 =========\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig, set_seed\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ========= Step 2: Configuration =========\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "output_dir = \"./finetuned_gemma-7b-it\"\n",
    "max_length = 128\n",
    "batch_size = 4\n",
    "gradient_accumulation = 4  # → effective batch size = 16\n",
    "num_epochs = 3\n",
    "seed = 42\n",
    "\n",
    "# ========= Step 3: Load and prepare dataset =========\n",
    "df = pd.read_excel(\"final_cleaned_8330.xlsx\")[[\"cleaned_parent\", \"cleaned_reply\"]].dropna()\n",
    "df = df.rename(columns={\"cleaned_parent\": \"post\", \"cleaned_reply\": \"reply\"})\n",
    "\n",
    "# Add prompt template\n",
    "df[\"text\"] = df.apply(\n",
    "    lambda row: f\"<|startoftext|>Assume you are a teacher or student in an online course forum. Please reply to this post: <Post> {row['post']} <Reply> {row['reply']}<|endoftext|>\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_texts, eval_texts = train_test_split(df[\"text\"], test_size=0.2, random_state=seed)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"text\": train_texts.tolist()}),\n",
    "    \"validation\": Dataset.from_dict({\"text\": eval_texts.tolist()})\n",
    "})\n",
    "\n",
    "# ========= Step 4: Load and configure tokenizer =========\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Add pad_token\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# ========= Step 5: Prepare QLoRA model =========\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# Optional: load without quantization\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ========= Step 6: Set training arguments and train =========\n",
    "set_seed(seed)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation,\n",
    "    num_train_epochs=num_epochs,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ========= Step 7: Save model and tokenizer =========\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a80a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========= Step 1: Load the fine-tuned model =========\n",
    "model_path = \"./finetuned_gemma-7b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# ========= Step 2: Load the original post data and run inference =========\n",
    "df = pd.read_excel(\"final_cleaned_8330.xlsx\")[[\"cleaned_parent\", \"cleaned_reply\"]].dropna()\n",
    "df = df.rename(columns={\"cleaned_parent\": \"post\", \"cleaned_reply\": \"true_reply\"})\n",
    "\n",
    "generated_replies = []\n",
    "for post in tqdm(df[\"post\"], desc=\"Generating...\"):\n",
    "    prompt = f\"<|startoftext|>Assume you are a teacher or student in an online course forum. Please reply to this post: <Post> {post} <Reply>\"\n",
    "    output = generator(prompt, max_new_tokens=80, temperature=0.7)[0][\"generated_text\"]\n",
    "    reply = output.split(\"<Reply>\")[-1].split(\"<|endoftext|>\")[0].strip()\n",
    "    generated_replies.append(reply)\n",
    "\n",
    "df[\"generated_reply\"] = generated_replies\n",
    "df.to_excel(\"gemma_generated_replies.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecefa4c",
   "metadata": {},
   "source": [
    "## Fine-tuning GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7032f87",
   "metadata": {
    "id": "H7LoMj4GA4n_"
   },
   "source": [
    "#  Fine-tuning a GPT-2 Replies-Generating Model w/ GPU For Free\n",
    "\n",
    "by [Max Woolf](http://minimaxir.com)\n",
    "modified by Zifeng Liu\n",
    "\n",
    "*Last updated: October 17th, 2024*\n",
    "\n",
    "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
    "\n",
    "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read my [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
    "\n",
    "\n",
    "To get started:\n",
    "0. Prepare the Mooc Posts Dataset(https://datastage.stanford.edu/StanfordMoocPosts/; Contact paepcke@cs.stanford.edu to get the data)\n",
    "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
    "2. Make sure you're running the notebook in Google Chrome.\n",
    "3. Run the cells below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a267a142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/homebrew/lib/python3.11/site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-25.1.1\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac590d99",
   "metadata": {
    "id": "KBkpRgBCBS2_"
   },
   "outputs": [],
   "source": [
    "!pip install -q gpt-2-simple\n",
    "import gpt_2_simple as gpt2\n",
    "from datetime import datetime\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49919662",
   "metadata": {
    "id": "Bj2IJLHP3KwE"
   },
   "source": [
    "## GPU - eight NVIDIA A100 GPUs utilizing Python 3\n",
    "\n",
    "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
    "\n",
    "You can verify which GPU is active by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "488923c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUmTooTW3osf",
    "outputId": "a5de2eba-5a13-4dae-92e2-1e756dae344c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a74b87",
   "metadata": {
    "id": "0wXB05bPDYxS"
   },
   "source": [
    "## Downloading GPT-2\n",
    "\n",
    "If you're retraining a model on new text, you need to download the GPT-2 model first.\n",
    "\n",
    "There are three released sizes of GPT-2:\n",
    "\n",
    "* `124M` (default): the \"small\" model, 500MB on disk (We used this one!!).\n",
    "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
    "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
    "* `1558M`: the \"extra large\", true model. Will not work if a K80/P4 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
    "\n",
    "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
    "\n",
    "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
    "\n",
    "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae346a3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8wSlgXoDPCR",
    "outputId": "b756405b-70f3-4014-8d19-6a06a03d926e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, ?it/s]                                                         \n",
      "Fetching encoder.json: 1.05Mit [00:01, 769kit/s]                                                    \n",
      "Fetching hparams.json: 1.05Mit [00:00, ?it/s]                                                       \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [02:03, 4.03Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, ?it/s]                                                   \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 1.95Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 1.83Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "gpt2.download_gpt2(model_name=\"124M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ccd0f5",
   "metadata": {
    "id": "N8KXuKWzQSsN"
   },
   "source": [
    "## Mounting Google Drive\n",
    "\n",
    "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
    "\n",
    "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a603a00b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "puq4iC6vUAHc",
    "outputId": "d52111c9-3927-4057-e786-bc6af85cda0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "gpt2.mount_gdrive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef546a92",
   "metadata": {
    "id": "BT__brhBCvJu"
   },
   "source": [
    "## Uploading a Text File to be Trained to Colaboratory\n",
    "\n",
    "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
    "\n",
    "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
    "\n",
    "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b96beb",
   "metadata": {
    "id": "6OFnPCLADfll"
   },
   "outputs": [],
   "source": [
    "# file_name = \"posts_data.txt\"\n",
    "file_name = \"./data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b83cdc",
   "metadata": {
    "id": "HeeSKtNWUedE"
   },
   "source": [
    "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a477d04f",
   "metadata": {
    "id": "-Z6okFD8VKtS"
   },
   "outputs": [],
   "source": [
    "gpt2.copy_file_from_gdrive(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37363f",
   "metadata": {
    "id": "LdpZQXknFNY3"
   },
   "source": [
    "## Finetune GPT-2\n",
    "\n",
    "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
    "\n",
    "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
    "\n",
    "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
    "\n",
    "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
    "\n",
    "\n",
    "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
    "* **`sample_every`**: Number of steps to print example output\n",
    "* **`print_every`**: Number of steps to print training progress.\n",
    "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
    "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
    "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f52db17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeXshJM-Cuaf",
    "outputId": "4731bca8-2465-49bb-b51f-a3a2096f9304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\liuzifeng\\Dropbox (UFL)\\Papers\\IHE_Explainable AI\\Data\\.venv\\lib\\site-packages\\gpt_2_simple\\gpt_2.py:215: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\liuzifeng\\Dropbox (UFL)\\Papers\\IHE_Explainable AI\\Data\\.venv\\lib\\site-packages\\gpt_2_simple\\gpt_2.py:241: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Loading checkpoint models\\124M\\model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models\\124M\\model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 2568833 tokens\n",
      "Training...\n",
      "[10 | 172.51] loss=3.13 avg=3.13\n",
      "[20 | 389.14] loss=3.35 avg=3.24\n",
      "[30 | 610.21] loss=3.07 avg=3.18\n",
      "[40 | 821.72] loss=3.07 avg=3.15\n",
      "[50 | 1045.59] loss=3.19 avg=3.16\n",
      "[60 | 1269.41] loss=3.31 avg=3.19\n",
      "[70 | 1480.31] loss=3.09 avg=3.17\n",
      "[80 | 1689.91] loss=3.05 avg=3.16\n",
      "[90 | 1896.71] loss=3.17 avg=3.16\n",
      "[100 | 2093.66] loss=3.30 avg=3.17\n",
      "[110 | 2280.89] loss=3.16 avg=3.17\n",
      "[120 | 2477.11] loss=3.07 avg=3.16\n",
      "[130 | 2679.44] loss=3.11 avg=3.16\n",
      "[140 | 2896.41] loss=3.07 avg=3.15\n",
      "[150 | 3041.04] loss=3.16 avg=3.15\n",
      "[160 | 3190.72] loss=3.15 avg=3.15\n",
      "[170 | 3388.85] loss=2.86 avg=3.13\n",
      "[180 | 3557.09] loss=2.81 avg=3.11\n",
      "[190 | 3721.12] loss=3.09 avg=3.11\n",
      "[200 | 3886.12] loss=3.15 avg=3.11\n",
      "======== SAMPLE 1 ========\n",
      "___name__\n",
      "\"I like the question about what is called from a different period in the history of human rights to the point that it allows people to talk about some of the human rights issues that were discussed.  What are your thoughts on it?\"\n",
      "\"It is just wrong. I like it.\"\n",
      "\"Hello guys, I just can't do the calculations either. I was going to say I think I can add my own observations.  I got 0 and got 5. So please be kind!\"\n",
      "\"I want to say a humble favor to the first person who asked this question:  I want you to do the math.  The math was done without any real understanding.  It is just an easy way to understand what other people think.  It's really easy.\"\n",
      "\"You are welcome to make a note of any mistakes and questions that we may have!  Also, please use the notation given in the videos and other materials, such as \\A1\\\"\".\"\"\"\n",
      "\"I feel that this was a little slow and I think it wasn't really about the time.\"\n",
      "\"Hello, I have always been interested in human rights and why, I never thought that there were a lot of different ways of saying that some people who are not members of any religion are guilty of not being able to access justice.\"\n",
      "\"Yes, that's correct. I'm a liberal.\"\n",
      "\"Yes, that's not really a problem because the reason behind it is that I love the word freedom. I love the idea that the world is not made of us all, and that every man with a brain needs to be free and able to do anything he wants to improve.\"\n",
      "\"Hello.  The idea that we could make a difference by doing something that seems like it doesn't need to be done and there is no obligation to do it.  The idea that somehow we would see changes that are not happening in our culture is not the same idea as how we might make a difference by teaching students to take risks and make mistakes.\"\n",
      "I see that it can be done in any way. I'm grateful for the challenge, and hope to be able to do it this year.\n",
      "\"Yes, but the number of people that are aware of the idea and have made the comments is not large enough to overcome the misconception.\"\n",
      "\"Yes, I am.  At 6:51 the dot card starts flashing at 6:21 and a little later, at 8:52, the dot card is flipped and continues to flash for an hour before I realize it is not being used for the number of persons that are concerned with taking risks and making mistakes.  At that point, I believe it will be obvious to everyone in the room, and I'll give you an explanation.  It is not very obvious.  In fact, it's not very hard.  It is not until the student does a thought experiment or is asked to look something up that he is given the instructions to do something.\"\n",
      "\"Thanks!  I really appreciate the effort of the students making this distinction but I would like to clarify.  I think that the people who made the distinction in the first place are all doing it with the knowledge they are doing and for the students to not understand that it's not really there when you take a risk.\"\n",
      "It seems to me like the students understand that there will be consequences.\n",
      "\"What do I do to resolve this confusion?  Just write down the names of the people that are talking about the questions.  I will then explain.  I will also ask them what happens when I make the mistake of being too much of a risk.  But if it is a mistake, then they should tell me why we make the mistake.  I guess I have an important lesson for them.\"\n",
      "Is it more likely to occur more often in an African-American population where black lives and poverty are the major obstacles?\n",
      "\"Hi, I'm a native of the Upper Midwest which is where this post was posted. I'm on my first day at college.  There are many things going on and I'm excited to help out with all the work we can do.  I'm curious if there's something that's getting my attention more than just the material on page 4 (saying I'm new to the topic in this thread)  Thank you for joining us!\"\n",
      "\"I would prefer to know that there were 2-3 years of student service, since I'm not sure I would ever want to hear the full discussion. I've heard that there were 2-3 of them.  I would also wish to know that there were 5 or more students that participated in the study.  _x0007__x0007_Overall, I would prefer to hear these discussions as well as some number discussions.  I'd like to know that the students who were deemed not on track (that were deemed 'not a student' anyway)\"\n",
      "\"I want to see what is the number of people that\n",
      "\n",
      "[210 | 4087.08] loss=2.94 avg=3.10\n",
      "[220 | 4256.09] loss=3.16 avg=3.11\n",
      "[230 | 4430.55] loss=3.08 avg=3.11\n",
      "[240 | 4609.01] loss=2.92 avg=3.10\n",
      "[250 | 4816.85] loss=2.97 avg=3.09\n",
      "[260 | 5037.93] loss=3.03 avg=3.09\n",
      "[270 | 5217.06] loss=2.99 avg=3.08\n",
      "[280 | 5363.95] loss=3.08 avg=3.08\n",
      "[290 | 5561.71] loss=3.18 avg=3.09\n",
      "[300 | 5714.61] loss=2.91 avg=3.08\n",
      "[310 | 5911.73] loss=2.92 avg=3.08\n",
      "[320 | 6068.24] loss=2.90 avg=3.07\n",
      "[330 | 6288.52] loss=3.13 avg=3.07\n",
      "[340 | 6475.98] loss=2.75 avg=3.06\n",
      "[350 | 6618.25] loss=3.05 avg=3.06\n",
      "[360 | 6805.65] loss=3.13 avg=3.06\n",
      "[370 | 7035.03] loss=3.04 avg=3.06\n",
      "[380 | 7282.54] loss=2.76 avg=3.05\n",
      "[390 | 7510.65] loss=3.01 avg=3.05\n",
      "[400 | 7656.03] loss=2.85 avg=3.04\n",
      "======== SAMPLE 1 ========\n",
      ". The main benefit of learning the course is the opportunity to have a discussion about the value of mistakes and develop a brain growth mindset.\n",
      "I would start this post talking about how we can build more brain networks of our own around mistakes.  You can also think of brain networks that look like these:\n",
      "So where does learning come from?  It's pretty interesting to me that mistakes were more important than thinking about them.  Also, my college and other universities that focus on learning are also seeing the growth mindset.  You need to be very selective to use that kind of math teacher for that kind of course.\n",
      "I am interested in making it easier for students who are stressed and worried about how they are to be able to come back later to do it when they are more experienced.\n",
      "\"I am an old-school teacher but I find it great that the kids in my class can really learn math.  The new students will be able to see the mistakes of other students and will really understand that this math is more accessible to them._x0007__x0007_Also, I love the concept of sharing problems from an early age.\"\n",
      "\"I love this \\free-form\\\"\" classroom.  These kids are excited to explore and learn and are loving being challenged.  They have very little information about math and they enjoy being \\\"\"challenged\\\"\" in front of their peers.\"\"\"\n",
      "\"I believe many students have a deep sense that math is fun and engaging.  They enjoy exploring math (and finding the answers to problems!)_x0007_I also believe that it is great to have a variety of options in learning math.  I have found that, in my school as a whole, students sometimes choose to work with some of the same people (or a different background) who often bring many of the same ideas to the table to make a learning experience.\"\n",
      "\"I am excited to join this community and to share many things that I've learned through the course and with my peers.  _x0007__I just recently got a letter from my ex-husband that said that math is hard, and that he would let me stay in his school until I achieved a certain grade.  I wanted my ex-wife, who I am married to for 20 years, to be a parent for the next 20 years.  She was shocked and angry to see my ex-husband say that math was about finding the right formulas.  She didn't want it to be my life that I had failed.  She wanted me to get a special education certificate, but she knew that I was only part of the equation because of my ex-husband.  She said that she would teach math class for 20 years, if she didn't lose her ex-husband this early on!_x0007_I am excited to come on board with the course, learn, and get to know the students that I have and who I have enjoyed with this wonderful experience.  I'll be looking forward to trying to work with the other students to help me develop and grow my math brain.  Thank you!\"\"\"\n",
      "I'm afraid it all depends on being able to work in groups and with multiple learners. So, can we all become students in the same classroom when no other student is seeing the same task and ability/work? Also, how do you make your own learning environment accessible, if each class has some rule/rule or practice/suggestions that will give you a chance to add on.\n",
      "We should learn to talk and share problems. I'm going to ask my students to share our mistakes, their ideas and successes using this vocabulary and I will use this vocabulary repeatedly in practice too.\n",
      "\"I don't think we need to be constantly updating our brains to learn from our mistakes. If we have done research on using the brain as a vehicle for brain growth, I wonder if there is any benefit in adding more brains and teaching them differently.\"\n",
      "I am so looking forward to this online course. Thank you!\n",
      "\"As a parent, I have always held that parents were always the ones to let my child do math (even if they did not really know what it meant).  I have a child who is 6 and I know he's smart and I want him to be ready to do the math when he grows up.  Unfortunately, his parents don't want me to tell him what he can do as he is a child who wants his teacher to let him do his work.  I can see that he will be disappointed in me sometimes, but I have been able to pull that off, too.  The most impressive moment when I tell him that he can really do this work is the video in the second person.  He has very little time to show me the video, and it is clear that it has been over 1 hour long and he has finished it.  (Maybe I can learn him in this way by doing this together!) I like that about her.  The students who\n",
      "\n",
      "[410 | 7871.68] loss=3.06 avg=3.04\n",
      "[420 | 8077.62] loss=2.81 avg=3.04\n",
      "[430 | 8290.26] loss=2.88 avg=3.03\n",
      "[440 | 8488.85] loss=2.97 avg=3.03\n",
      "[450 | 8693.86] loss=3.03 avg=3.03\n",
      "[460 | 8903.26] loss=2.89 avg=3.03\n",
      "[470 | 9104.54] loss=2.09 avg=3.00\n",
      "[480 | 9312.07] loss=2.96 avg=3.00\n",
      "[490 | 9524.69] loss=2.95 avg=3.00\n",
      "[500 | 9727.66] loss=2.70 avg=2.99\n",
      "Saving checkpoint\\run1\\model-500\n",
      "[510 | 9930.19] loss=2.81 avg=2.99\n",
      "[520 | 10122.36] loss=2.97 avg=2.99\n",
      "[530 | 10319.89] loss=2.94 avg=2.99\n",
      "[540 | 10518.41] loss=2.80 avg=2.98\n",
      "[550 | 10711.77] loss=2.87 avg=2.98\n",
      "[560 | 10904.75] loss=3.07 avg=2.98\n",
      "[570 | 11093.03] loss=2.97 avg=2.98\n",
      "[580 | 11279.97] loss=2.84 avg=2.98\n",
      "[590 | 11465.23] loss=3.15 avg=2.98\n",
      "[600 | 11651.31] loss=2.70 avg=2.98\n",
      "======== SAMPLE 1 ========\n",
      " know.  I know what \\\"\"trend changes.\\\"\"\"\"\"\n",
      "\"Thanks for this great class, it really helped me understand how to make sense of this number, and the way it can be expressed.  Also, I'm really interested in the number of ways in which people can solve it.  Thanks for an amazing class!\"\n",
      "\"That depends on how many times I've heard from people who've worked with the same thing. _x0007_My experience has been that the students in one group seemed to be doing the exact same thing.  The other students seemed to be doing what their mother wanted them to do._x0007__x0007_I think it depends on if the students have ever had a difficult upbringing or if they've had it as a \\wrong\\\"\" way of learning.  Sometimes the best way of doing something is to try some new ideas and try something new and try it until you can \\\"\"get\\\"\" something from the whole concept._x0007_I don't want to say we shouldn't try it, I think it's important to look at it as something totally different to the idea.\"\"\"\n",
      "\"I think the same thing occurs in the \\free\\\"\" learning environment.\\\"\"_x0007__x0007_\\\"\"The teacher needs to tell each student the \\\"\"how\\\"\" of each method.\\\"\"_x0007__x0007_\\\"\"Every student will understand the technique for a week.\\\"\"\"\"\"\n",
      "Thanks for the suggestion. The best way is to use some of the vocabulary that people are already familiar with. I'm not sure you can really find the words in the Oxford U.T. system.\n",
      "\"I loved the idea. The \\\"\"rule or set\\\"\" of decompose. This is really cool, for those who have had a difficult childhood. What is the best way to decompose these things like \\\"\"rule\\\"\" and \\\"\"set\\\"\"?_x0007_Thanks in advance for the great course!\"\"\"\n",
      "\"Hello all,_x0007_I am very pleased to find that I am not afraid to make mistakes. I am a student of your class. It made my day! I am enjoying the course and would like to learn more from it. Any suggestions or opinions regarding mistakes or mistakes in class are welcome!_x0007__x0007_Best of Luck!\"\n",
      "I would like to add that I really appreciate the work of Anne Boesch and her fellow students. Her passion for the subject of the course and this course are beyond me. I have found the materials which make her material really interesting...and very interesting.\n",
      "\"Well, the research of this is fascinating, and I've already been thinking about that!\"\n",
      "It sounds like you mentioned some research done. I know that the brains in rats are growing, but do they make more connections and learn as much as do you?\n",
      "\"I am looking forward to learning from the students who shared the same interest in math.  I was surprised when the teacher mentioned that Math was \\free\\\"\" and that everyone can do it and be creative/different.  A very cool response.  (Also, I was very curious to hear how the students described Math as \\\"\"free\\\"\" and \\\"\"free to do whatever you want and be creative._x0007_ I would love to see the students with developmental dyslexia make a career as an educator, and have a fun working-life.\"\"\"\n",
      "I see my students in the same boat... We need to learn to be friendly with one another!\n",
      "Here is a link to an interview with a Math Coach at: http://www.jstern.org/news/business-business-economics-in-jstern/\n",
      "\"I'm taking this class to see if there are a couple of options you could share with others.  I'm going to ask you if you would like to learn how I have used the Stanford Algebra Online Course.  This is the Algebra for All the Years Program.  This is why I teach algebra in our class.\"\n",
      "\"Dear Stanford,_x0007__x0007_My name <nameRedac_<anon_screen_name_redacted>> from California,I am a Data Scientist in Data Analytics @ University of Chicago..I am taking this course to learn and apply some ideas about the differentiating between data and theory in the area of heterogeneous data in a way I feel will improve my skills in Data Analysis.\"\n",
      "\"At first, I think that this is a great idea, as learning from mistakes helps to improve the skills of others. Then, when I start making a mistake, I begin to \\see\\\"\" why. In the beginning of my own learning process I think of what was said.  That's exactly what I have done: tried to do the same thing, but different ways, and changed the way I\n",
      "\n",
      "[610 | 11892.34] loss=2.65 avg=2.97\n",
      "[620 | 12105.54] loss=3.05 avg=2.97\n",
      "[630 | 12283.79] loss=2.64 avg=2.96\n",
      "[640 | 12496.69] loss=3.19 avg=2.97\n",
      "[650 | 12705.41] loss=2.77 avg=2.96\n",
      "[660 | 12884.04] loss=2.76 avg=2.96\n",
      "[670 | 13020.37] loss=2.87 avg=2.96\n",
      "[680 | 13197.88] loss=2.75 avg=2.95\n",
      "[690 | 13389.94] loss=2.73 avg=2.95\n",
      "[700 | 13572.61] loss=2.80 avg=2.95\n",
      "[710 | 13748.67] loss=2.78 avg=2.94\n",
      "[720 | 13920.08] loss=2.64 avg=2.94\n",
      "[730 | 14085.20] loss=2.87 avg=2.94\n",
      "[740 | 14247.28] loss=2.69 avg=2.93\n",
      "[750 | 14410.25] loss=2.63 avg=2.93\n",
      "[760 | 14574.31] loss=2.82 avg=2.92\n",
      "[770 | 14737.39] loss=2.82 avg=2.92\n",
      "[780 | 14903.88] loss=2.70 avg=2.92\n",
      "[790 | 15088.73] loss=2.88 avg=2.92\n",
      "[800 | 15267.82] loss=2.79 avg=2.91\n",
      "======== SAMPLE 1 ========\n",
      "x0007_I would like to say a word about the video lectures. Some of them are easy enough for children to understand but more so for adults.  _x0007_There are also a few things students need to know about math and that is the most important aspect of this class.  I have seen some of the comments that these videos are really helpful._x0007_The video lectures are also very well organized in terms of materials and concepts. For example they are all very succinct and concise.\"\n",
      "\"I have been thinking, when we have an intervention group of students - how can we say what the groups are?_x0007__x0007_Also, how does grouping, the intervention, affect the learning outcomes?_x0007_Will our intervention group make a difference? Can we say that group did the intervention (e.g., we had a parent with dyslexia vs ADHD or our age group) or that we were just given a fixed amount of time in our classroom (8-15mins or 12-15 mins of intervention)?_x0007_Is it something we have to do and then put into the intervention or is it something we will never do?_x0007_I think grouping is essential to both grouping and educating each other about mistakes and learning.\"\n",
      "I loved this very carefully written sentence!!\n",
      "\"I think all students need to be taught to do this. I teach in small groups. I try to encourage the student to find a problem they are familiar with and then I tell them that a person can make a whole math brain and grow it up. This is done to show that we all learn differently. I also encourage this as well, encouraging all students to try to do the same problems. Everyone does this for a variety of reasons. I can see why some students are successful and some of the success is due to ability grouping. I also know that I have a lot of children and I don't make as much. The kids I do think I help if the teacher knows the students will make mistakes and that all too often there are negative messages that students are not as good at math because they are the same age. They become a little anxious over the next day and I feel badly affected. I think many of the problems are caused by anxiety related to the teachers experience. I do think the learning opportunities the school provides is valuable and have noticed a shift toward more 'friendly' math experiences, or at least an effort on it at the beginning. This is where parents and teachers can show they would benefit from having this type of a learning environment in their child's education.\"\n",
      "\"I would suggest that in the case of fixed mindset grouping (i.e., grouping a group for one year and a half of students who do not understand geometry, for example), this is a good way to try to shift the paradigm in the minds of the children.  In the case of groups with low ability, I would like to emphasize the importance of being prepared to try different methods.  If we all take responsibility for our own learning - that is just a part of learning, not something we should do - what is most important - that of making the time we take to correct.  Learning happens at its stages!\"\n",
      "\"I loved the way they used it in front of the class to talk about their math knowledge. I think that makes for a great lesson to follow, because I know that there are so many students who can relate the dots. I think it may help the teacher figure out how the students have been grouped, because sometimes there are students who are not really that \\good\\\"\" at math, the other students still do not get the same message. This way, the teacher can talk to the students about what they are learning and what they need to do differently.\"\"\"\n",
      "\"I have the same feeling as you about the ability of students. I think it's very damaging for educators and the students. I can think of some students who are excellent at math but not good at other areas of their learning (or have never been taught math in their life). The fact is that students can't move beyond basic concepts and methods (e.g., multiplying a line by a die). Students then lose all of the fun of reading about how their brain is growing and making connections. I also have a feeling that kids who are good at math will not take those courses because I think they either don't understand the geometry or haven't the skill to understand how the numbers should be produced. I believe there are many people who believe in \\freeing\\\"\" kids to take up the challenge, and I know students who are extremely talented at math, but have not yet mastered that and I think it is good for their mental growth.\"\"\"\n",
      "I agree and look forward to the discussion sections of this course.\n",
      "\"I have often considered myself smart because I have always had the belief that the only way I would ever pass the A's (or the C's) is if I had\n",
      "\n",
      "[810 | 15470.46] loss=2.53 avg=2.91\n",
      "[820 | 15638.60] loss=2.77 avg=2.91\n",
      "[830 | 15804.80] loss=2.79 avg=2.90\n",
      "[840 | 15968.22] loss=2.96 avg=2.90\n",
      "[850 | 16128.41] loss=2.64 avg=2.90\n",
      "[860 | 16283.47] loss=2.72 avg=2.90\n",
      "[870 | 16438.81] loss=2.76 avg=2.89\n",
      "[880 | 16594.51] loss=2.69 avg=2.89\n",
      "[890 | 16752.75] loss=2.47 avg=2.88\n",
      "[900 | 16910.01] loss=2.74 avg=2.88\n",
      "[910 | 17067.91] loss=3.08 avg=2.88\n",
      "[920 | 17225.60] loss=2.30 avg=2.87\n",
      "[930 | 17381.95] loss=2.52 avg=2.87\n",
      "[940 | 17538.03] loss=2.72 avg=2.87\n",
      "[950 | 17695.11] loss=2.71 avg=2.86\n",
      "[960 | 17853.44] loss=2.53 avg=2.86\n",
      "[970 | 18030.07] loss=2.55 avg=2.85\n",
      "[980 | 18179.19] loss=3.00 avg=2.86\n",
      "[990 | 18311.24] loss=2.50 avg=2.85\n",
      "[1000 | 18480.36] loss=2.59 avg=2.85\n",
      "Saving checkpoint\\run1\\model-1000\n",
      "WARNING:tensorflow:From c:\\Users\\liuzifeng\\Dropbox (UFL)\\Papers\\IHE_Explainable AI\\Data\\.venv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1068: remove_checkpoint (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess,\n",
    "              dataset=file_name,\n",
    "              model_name='124M',\n",
    "              steps=1000,\n",
    "              restore_from='fresh',\n",
    "              run_name='run1',\n",
    "              print_every=10,\n",
    "              sample_every=200,\n",
    "              save_every=500\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5821ab4",
   "metadata": {
    "id": "IXSuTNERaw6K"
   },
   "source": [
    "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
    "\n",
    "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3cd8b4",
   "metadata": {
    "id": "VHdTL8NDbAh3"
   },
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e143f0",
   "metadata": {
    "id": "qQJgV_b4bmzd"
   },
   "source": [
    "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185964d",
   "metadata": {
    "id": "pel-uBULXO2L"
   },
   "source": [
    "## Load a Trained Model Checkpoint\n",
    "\n",
    "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4e0a51",
   "metadata": {
    "id": "DCcx5u7sbPTD"
   },
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2acc5",
   "metadata": {
    "id": "RTa6zf3e_9gV"
   },
   "source": [
    "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
    "\n",
    "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e50c22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fxL77nvAMAX",
    "outputId": "f85a9336-7daa-49d0-b9ab-9266bf415fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\liuzifeng\\Dropbox (UFL)\\Papers\\IHE_Explainable AI\\Data\\.venv\\lib\\site-packages\\gpt_2_simple\\gpt_2.py:401: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "Loading checkpoint checkpoint\\run1\\model-1000\n",
      "INFO:tensorflow:Restoring parameters from checkpoint\\run1\\model-1000\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b95254",
   "metadata": {
    "id": "ClJwpF_ACONp"
   },
   "source": [
    "## Generate Text From The Trained Model\n",
    "\n",
    "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dac41143",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RNY6RBI9LmL",
    "outputId": "0fd0a16e-58fe-4bf1-8232-f4e6059a56bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"In the first week, one of the essays was very unhelpful. I didn't like the language, but my colleagues had the same issue. I think this may be due to the way the instructor did it. I can't blame her for that. She didn't approach the text with the exact same eyes, but she approached it with the same intent. She didn't give the reader a sense of competence, but she made a point to make sure that the author didn't make a mistake. _x0007_I think that I am going to have to get over this and read the essay again. _x0007_I was not happy with where I was on the essay, but I think I have to get over it somehow. I also have to read the essay again because I have already finished the essay. As you may know, I am not following the course closely. I am looking forward to reading more essays and I hope that I can make it through the course.\"\n",
      "\"I think the way you approach the peer grading is a problem that you have to deal with at your own pace. I think that the discussion is really important. You want to see that the reviewers are more interested in helping you than guiding you along the way. I think that if you are able to talk through your writing, if you are able to get feedback from the other reviewers, then any feedback will help you to improve your writing. If you are able to talk about how you think your work is better, then the feedback from the others will help you to get better. If you have to keep trying, you will not get better.\"\n",
      "\"Each week, I will be reviewing 2 essays from the readings for each section, so I don't get stuck into the same problem (that I am getting). If you have any suggestions, I will add them to the list to see what properties you will be getting.\"\n",
      "\"I was struck by the response of your \\That's how it works - you just make the best of it\\\"\", and also by the fact that the authors are actually describing the properties, rather than a list of them. I'm not sure how this makes sense - and I am sure it will make a lot of sense in a scientific paper, but for me the most interesting part was to see the authors write down their property terms, in a language, rather than just a list of them. I hope this helps.\"\"\"\n",
      "\"As a writer I generally have a hard time understanding the intent of a manuscript and writing process. For example, I have yet to write a note of recommendation to the editor about a paper I have reviewed. I may have reviewed a paper using some of the statistics methods described in the book, but I haven't published it. I hope for the best, and I'm hopeful that others will follow suit.\"\n",
      "\"I think the lack of a peer review is an issue that needs to be addressed before the writing process can be improved. In this, I agree, as I have found, there is a lot of time wasted before the initial review._x0007__x0007_I also doubt that the reviewer would feel the need to separate the writing from the process. I know that grading is a great way to learn and use words. It is also very important to have a good balance between the writing and the process.\"\n",
      "\"I'm not sure how I can read the concept and test outcomes, but I do have the same problem: I don't have a good view of the situation to evaluate the paper, and I can't see the \\progress\\\"\" on the screen. What is there to do? What's the point of reviewing a paper if I'm editing it in some way? _x0007__x0007_I understand that I can only see the progress and the changes I make to the paper, but I don't know how to read the \\\"\"progress\\\"\" of the \\\"\"test results\\\"\". I don't know how to give feedback in writing, but I'm afraid to not say I'm 100% sure the answer to that is no. If I could read the concept and test outcomes, I would be very satisfied with the \\\"\"progress\\\"\".\"\"\"\n",
      "\"I wish that you could have students, those of us with strong math background, who would be willing to give up all their time and effort to finish a \\good\\\"\" paper. I think this will be a great way to encourage more young people to give up and come to an understanding of math and math. I think that math is a subject that we will all learn a lot, and some of us may not have the mathematical background to complete a successful course in math, but I think we are going to need to learn a lot of math to get to an understanding of math.\"\"\"\n",
      "I think the problem here is that one of the two reviewers is supposed to do a quick peer review to see if the book was reviewed._x0007__x0007\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612281a7",
   "metadata": {
    "id": "oF4-PqF0Fl7R"
   },
   "source": [
    "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
    "\n",
    "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
    "\n",
    "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
    "\n",
    "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
    "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
    "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
    "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
    "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
    "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1ebcd8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "8DKMc0fiej4N",
    "outputId": "e088751c-f785-4f06-bec5-4b06bbf6c110"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the pervasive idea that to be good at Math, is somehow, to be an otherwise absent-minded, Geeky Genius who can\\'t relate to \\regular people.  _x0007__x0007_I also think that some of the higher-school students who have been told they are good at Math are often the ones who are no longer \\\\\"\"math-dumb\\\\\"\" and are forced to think of Math as something that is \\\\\"\"easy\\\\\"\".   It\\'s hard to \\\\\"\"get\\\\\"\" what you are actually learning.\"\"\"\\n\"Hello everyone,_x0007__x0007_I am an elementary school teacher in a rural, predominantly African-American school district in Kansas City, MO. I teach 4-year-olds who have been born in the United States and have been taught in a different culture from their family. I am hoping to create a safe environment where all students feel safe enough to express their opinions about math, as well as some of the positive stories I hope to share._x0007__x0007_I am taking this course to make it easier for kids to feel comfortable with math. I am excited about how it will help them to feel like they \\\\\"\"can\\\\\"\" do math.\"\"\"\\n\"I don\\'t think, overall, students are quite as \\\\free\\\\\"\" as they are with math or math-related',\n",
       " \"the pervasive idea that to be good at Math, is somehow, to be an otherwise absent-minded, Geeky Genius who can't relate to \\regular people. _x0007__x0007_I would like to join in on the discussion with the fewest of the many things we have discussed.  _x0007__x0007_        _x0007__x0007_        _x0007_                                                                                                                                                                                      \",\n",
       " 'the pervasive idea that to be good at Math, is somehow, to be an otherwise absent-minded, Geeky Genius who can\\'t relate to \\regular people. I mean, I work with students who are generally not \\\\good at Math\\\\\"\". It\\'s working to make the math fun for them. If it is not fun, then it isn\\'t interesting. I want students to see how they can do math in a different way.\"\"\"\\n\"I would reinforce that I am not perfect, that the way I do math is not what I am supposed to do.  If I can\\'t see the problem, I don\\'t know what is the problem.  I will not let students feel bad about making mistakes, because I am trying to help the students learn and make mistakes.  I hope to use some of the techniques I teach in the classroom.\"\\nIt was really freeing to see how many different ways people saw the same problem. It was also nice to see how different people saw the same number and could see different solutions. It was also nice to hear how the students were able to pick and choose the right solutions and combine them.\\n\"As a math teacher, I have a small group that I work with who really like to \\\\get their math together\\\\\"\". I have them present their work and ideas to the class, and then use a variety of tools to help them re-te',\n",
       " 'the pervasive idea that to be good at Math, is somehow, to be an otherwise absent-minded, Geeky Genius who can\\'t relate to \\regular people.  It is so sad to hear of what these students have to say and the real impact that they have on their community.  I\\'m seeing how they view themselves in different ways, from the poor, to the poor, to the poor, to the poor, to the poor, to the poor, to the poor, to the poor, to the poor, to the poor.  I am very sad to hear them say they are so good at Math that they don\\'t want to do it, but they are not good at Math because they are not involved in the community that they are in._x0007_I\\'m not sure how to respond to this,\"\"\"\\n\"I think this is a huge step forward in educating students to understand that there is more than one way to solve a problem.  I agree with the research being stated that 60% of students believe that there is more than one way to solve a problem.  I think it is important that students have the opportunity to participate in the learning process and to be exposed to other students\\' ideas and ways of solving a problem.  I found that the more I listened and understood the process, the more I understood why it worked, and the more I became aware of',\n",
       " 'the pervasive idea that to be good at Math, is somehow, to be an otherwise absent-minded, Geeky Genius who can\\'t relate to \\regular people. _x0007_I believe that this interview is telling about the ways that the culture has prevailed in our society. _x0007_The culture of the class is very heterogeneous - girls are viewed as the \\\\other\\\\\"\" and boys as the \\\\\"\"good\\\\\"\" students. This is a very powerful message._x0007__x0007_I think that the girls that were working in the classroom were taught to be \\\\\"\"math loving\\\\\"\". They were taught that they were worth learning. They were taught to think differently._x0007__x0007_I also loved that they were excited about math and were very proud about how they were learning._x0007__x0007_They really liked Math. They really believed that it was a \\\\\"\"unfortunate\\\\\"\" part of the class. They were very proud that they were getting it and they liked it. They were very proud about how it was being done and the way that they were learning.\"\"\"\\nI don\\'t think it\\'s fair to say that the \\\\free\\\\\"\" Math classes are the best in the world. I think the students in the class really feel that they are free to do whatever they want to get to a']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2.generate(sess,\n",
    "              length=250,\n",
    "              temperature=0.7,\n",
    "              prefix=\"the pervasive idea that to be good at Math, is somehow, to be an otherwise absent-minded, Geeky Genius who can't relate to \\regular people.\",\n",
    "              nsamples=5,\n",
    "              batch_size=5,\n",
    "              truncate=\"<|endoftext|>\",\n",
    "              include_prefix = False,\n",
    "              return_as_list=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4ea2f",
   "metadata": {
    "id": "zjjEN2Tafhl2"
   },
   "source": [
    "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
    "\n",
    "You can rerun the cells as many times as you want for even more generated texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6923485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e30a3bf4",
   "metadata": {
    "id": "Fa6p6arifSL0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interesting! How often we say those things to others without really understanding what we are saying. That must have been a powerful experience! Excellent! _x0007__x0007_It's interesting to note that all culture is about the nurturing of children. I don't know how we can get all the more children to be nurtured if we are talking about helping other children. It's not just a one-way street, but it is important to be able to do this throughout our life. _x0007__x0007_What are some of the things you are describing and why?_x0007__x0007_1. I believe that if you are good at math, and you are using your brain, you will be better at math. You will have more opportunities to learn, and you will be more likely to start off the year better than you did. _x0007__x0007_2. The more you are exposed to math, the more you will be able to understand it more easily. I don't think you have to be a math person to understand it. There is so much that can be learned through math that you can learn from. You can learn to understand math.\"\n",
      "\"I had a very hard time with the question of \\what does a \\free\\\"\" sound mean\\\"\". I think that there are two main categories of words that have an effect on the listener: \\\"\"free\\\"\" and \\\"\"non-\\\"\"._x0007_I don't think anyone should be penalized if they end up in a non-free phrase. I think that the first category is really a little bit more of a general phrase and has a lot to do with the subject or concept. _x0007_The second category is a more specific term which is just a general phrase. In this example, I want to say \\\"\"non-\\\"\". I like the \\\"\"non\\\"\" and like the general phrase because it makes the point that the speaker is talking about a thing rather than a specific phrase. _x0007_I think that the speaker is talking about a thing that the speaker is talking about. _x0007_I don't know how much I agree with her. I think that the important thing is to be able to get to the point where you are able to make sense of the statement and it is important to be able to make sense of it. _x0007__x0007_Thanks for the suggestions, I was wondering if you could give any examples that would help you with this \n",
      "\n",
      "\"The interview with the students showed a very high level use of math vocabulary for 3rd grade students. They repeatedly used the word 'decomposing' and the phrase 'breaking down' to describe what helped them to understand math. They were also able to accurately explain why decomposing and breaking down are so important in making math easier - because it \\helps you to make friendly numbers.\\\"\" I liked that the one student provided an excellent example of a friendly number (10) when Jo asked him what a friendly number was. Finally, I thought it was interesting that the students, for the most part, recognized that talking about math with others helps them to understand math more clearly because it allows them the opportunity to compare their solution methods with other students' solution methods and it brings out mistakes, which are so important to learning math. It is also noteworthy that one of the students noted that \\\"\"talking about math does and doesn't help\\\"\" to understand math. I wonder how it \\\"\"doesn't help\\\"\" for him?\"\"\"\n",
      "\"I was struck by the first \\in the box\\\"\" statement.  Yes, I was looking for the box, not the label.  But I wasn't sure what to do with what I had found.  I looked at the box, and I realized, \\\"\"it doesn't matter what the box says!\\\"\"  Students probably wouldn't have thought that; they don't have the \\\"\"box\\\"\" in their heads.  They certainly don't have the \\\"\"discussion\\\"\" which arises when we put forth the effort to solve a problem, and they don't seem to have the \\\"\"decompose\\\"\" vocabulary.\"\"\"\n",
      "\"I think there are many ways to approach math.  Think about it: you have a math teacher who is doing what she can to help you figure out the problem.  Other ways to approach math will vary.  I think there are two approaches, or more, that are valid.  In my experience, students struggle with the idea of the \\\"\"different ways to approach math.\\\"\"\"\"\"\n",
      "\"I loved how they used the word decompose.  They didn't give up and went with it.  It was so freeing and so refreshing to hear them say that they weren't afraid of math\"\n",
      "I think the dot card number talk is a great way to work out the number sense and tie in number sense. The number sense is very important to get students to see the number sense.\n",
      "\"I think it is very interesting and interesting to see the different perspectives on the same problem.  I think it also makes students realize how they are able to process different ideas and give different methods of solving the problem.  When you see it in an open house, everyone is open to experimentation and it will make you more confident in your process of solving the problem.\"\n",
      "\"The students were very positive about math and they seemed to enjoy the subject. They also seemed to enjoy their math classes. I think that's a very positive message and I would love to see how these kids would respond to that.\"\n",
      "I teach in a district where all of our math classes are now. I had a parent who told me that she was in the \\middle\\\"\" of the class and that everyone in the class was \\\"\"smart\\\"\". She thought it was great that they were \\\"\"not as dumb\\\"\" as they were. They were smarter.\n",
      "\"I like how Cathy's \n",
      "\n",
      "\"I'm Brazilian, but I read very well in English, so it would be good that all videos have subtitles._x0007__x0007_thank's\"\n",
      "I really enjoyed the video discussing a possible human rights framework for women's rights for their own rights and in particular for their own rights for their own rights. It was curious to see how women's rights were used in relation to other aspects of life.\n",
      "\"Hello,_x0007_I'll be playing the video for 2.5 on the website. I don't know how the subtitles got here, but if anything, I can't see the video, so thanks to all staff and students from Stanford for doing their work. I'm looking forward to the next video!\"\n",
      "\"I'm glad to see that the evidence is there for the benefits of girls and women being educated, but I'm concerned that more evidence will be presented of the benefits of educating girls and women. While a lot of the evidence about the benefits of education is anecdotal, there is evidence to suggest that it is beneficial.\"\n",
      "The time is the average time it takes you to download the videos.\n",
      "\"Hi,_x0007_Although I don't find many studies that specifically address the benefits of educating girls or boys, I want to try to highlight one randomized controlled trial that had a significant effect on preventing HIV/AIDS by adjusting for confounders. It was the pivotal study to determine whether the intervention could prevent or treat HIV/AIDS. A study of 16,000 participants in a community-based, open-ended HIV prevention program found that the intervention group had a significant reduction in risk of acquiring HIV or AIDS. I'm just taking this part of this course to give myself a little extra time to really understand the studies and the implications of this study. _x0007_As I understand it, the intervention group had the lowest mean weight loss (0.10 kg) compared to the control group, but there was no evidence of a beneficial effect. _x0007_I'm simply taking this part of this course to get me back to the statistical aspects of this study.\"\n",
      "\"In the book a lot of the statistics surrounding the life expectancy for women is still the same. It is a topic that is very important to understand. The data is important to understand because the following are the things that are important to understand:_x0007__x0007_1. In the lay population, life expectancy varies by age, and those older than 55 live longer._x0007_2. There is a difference between the current US life expectancy and the last \n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "bad allocation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m prefixes:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Generate text using the loaded GPT-2 model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mgpt2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mnsamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We want only one sample per prefix\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Generate one text at a time\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mreturn_as_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Since nsamples=1, take the first item\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(generated_text,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\liuzifeng\\Dropbox (UFL)\\Papers\\IHE_Explainable AI\\Data\\.venv\\lib\\site-packages\\gpt_2_simple\\gpt_2.py:479\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(sess, run_name, checkpoint_dir, model_name, model_dir, sample_dir, return_as_list, truncate, destination_path, sample_delim, prefix, seed, nsamples, batch_size, length, temperature, top_k, top_p, include_prefix)\u001b[0m\n\u001b[0;32m    477\u001b[0m     out \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun(output)\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 479\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcontext_tokens\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m    483\u001b[0m     generated \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\liuzifeng\\Dropbox (UFL)\\Papers\\IHE_Explainable AI\\Data\\.venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:972\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    969\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 972\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    974\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    975\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\Users\\liuzifeng\\Dropbox (UFL)\\Papers\\IHE_Explainable AI\\Data\\.venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1215\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1215\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1218\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\liuzifeng\\Dropbox (UFL)\\Papers\\IHE_Explainable AI\\Data\\.venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1395\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1395\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1398\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32mc:\\Users\\liuzifeng\\Dropbox (UFL)\\Papers\\IHE_Explainable AI\\Data\\.venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1402\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1401\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1403\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1404\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32mc:\\Users\\liuzifeng\\Dropbox (UFL)\\Papers\\IHE_Explainable AI\\Data\\.venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1384\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1383\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m-> 1384\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extend_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m   1386\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[1;32mc:\\Users\\liuzifeng\\Dropbox (UFL)\\Papers\\IHE_Explainable AI\\Data\\.venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1425\u001b[0m, in \u001b[0;36mBaseSession._extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extend_graph\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1424\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39m_session_run_lock():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1425\u001b[0m     \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExtendSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: bad allocation"
     ]
    }
   ],
   "source": [
    "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
    "\n",
    "with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "    prefixes = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['Prefix', 'Generated Text'])\n",
    "i = 0\n",
    "for prefix in prefixes:\n",
    "    # Generate text using the loaded GPT-2 model\n",
    "    generated_text = gpt2.generate(sess,\n",
    "                                   length=500,\n",
    "                                   temperature=0.7,\n",
    "                                   prefix=prefix,\n",
    "                                   nsamples=1,  # We want only one sample per prefix\n",
    "                                   batch_size=1,  # Generate one text at a time\n",
    "                                   return_as_list=True\n",
    "                                   )[0]  # Since nsamples=1, take the first item\n",
    "    \n",
    "    if(i%100==0):\n",
    "        print(generated_text,'\\n')\n",
    "    \n",
    "    i = i + 1\n",
    "\n",
    "    # Create a new DataFrame for the current row\n",
    "    new_row = pd.DataFrame({'Prefix': [prefix], 'Generated Text': [generated_text]})\n",
    "\n",
    "    # Concatenate the new row to the existing DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# Set the filename with the current timestamp\n",
    "filename = f'gpt2_gentext0.7_{datetime.utcnow():%Y%m%d_%H%M%S}.xlsx'\n",
    "df.to_excel(filename, index=False)\n",
    "\n",
    "# gpt2.generate_to_file(sess,\n",
    "#                       destination_path=gen_file,\n",
    "#                       length=500,\n",
    "#                       temperature=0.7,\n",
    "#                       nsamples=100,\n",
    "#                       batch_size=20\n",
    "#                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afb06cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the filename with the current timestamp\n",
    "filename = f'gpt2_gentext0.7_{datetime.utcnow():%Y%m%d_%H%M%S}.xlsx'\n",
    "df.to_excel(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e07a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a734e5da",
   "metadata": {
    "id": "0-LRex8lfv1g"
   },
   "outputs": [],
   "source": [
    "# may have to run twice to get file to download\n",
    "files.download(gen_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29aad1",
   "metadata": {
    "id": "QQAN3M6RT7Kj"
   },
   "source": [
    "## Generate Text From The Pretrained Model\n",
    "\n",
    "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
    "\n",
    "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d75057",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "hsUd_jHgUZnD",
    "outputId": "4e0c8a3f-3527-41c4-e3fe-3357f3f8f6c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 354Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 131Mit/s]                                                    \n",
      "Fetching hparams.json: 1.05Mit [00:00, 279Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:23, 131Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 380Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 2.10Mit [00:00, 226Mit/s]                                                 \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 199Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "model_name = \"774M\"\n",
    "\n",
    "gpt2.download_gpt2(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283adda7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "BAe4NpKNUj2C",
    "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0828 18:37:58.571830 139905369159552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model models/774M/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.load_gpt2(sess, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e9536",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "-xInIZKaU104",
    "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The secret of life is that it's really easy to make it complicated,\" said Bill Nye, the host of the popular science show \"Bill Nye the Science Guy.\" \"And this is one of the reasons why we all need to be smarter about science, because we can't keep up with the amazing things that are going on all the time.\"\n",
      "\n",
      "While Nye is correct that \"everything that's going on all the time\" is making the world a better place, he misses the point. This is not\n",
      "====================\n",
      "The secret of life is in the rhythm of the universe. It's not a mystery. It's not a mystery to me. It's the nature of the universe. It's the beauty of the universe. It's the way the universe works. It's the way the universe is. It's the way the universe is going to work. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way\n",
      "====================\n",
      "The secret of life is in the universe.\n",
      "\n",
      "\n",
      "-\n",
      "\n",
      "The Red Devil\n",
      "\n",
      "It's the end of the world as we know it, and the only thing that can save us is a band of super-powered individuals known as the Red Devil.\n",
      "\n",
      "\n",
      "The Red Devil is a group of super-powered individuals who are seeking the secret of life and the only way they know how to do it is by taking on the roles of a variety of different super-powered individuals, each of which has their own\n",
      "====================\n",
      "The secret of life is in the mixing of the elements, and it is the mixing of the elements that makes life possible.\"\n",
      "\n",
      "But in the world of food science, the idea of a \"complex\" or \"complexity\" is almost entirely imaginary.\n",
      "\n",
      "As a scientist, I'm fascinated by the question of how life first began.\n",
      "\n",
      "It's the question that drives my work and the work of the scientists who work on it.\n",
      "\n",
      "My current research is exploring how microbes work in the first moments\n",
      "====================\n",
      "The secret of life is the journey of life, the search for the truth.\n",
      "\n",
      "4.4.2. The last thing you know\n",
      "\n",
      "There is nothing more important than the last thing you know.\n",
      "\n",
      "4.4.3. The little things that make all the difference\n",
      "\n",
      "The little things that make all the difference.\n",
      "\n",
      "4.4.4. The truth is the best teacher\n",
      "\n",
      "The truth is the best teacher.\n",
      "\n",
      "4.4.5. The truth is what\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess,\n",
    "              model_name=model_name,\n",
    "              prefix=\"The secret of life is\",\n",
    "              length=100,\n",
    "              temperature=0.7,\n",
    "              top_p=0.9,\n",
    "              nsamples=5,\n",
    "              batch_size=5\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667697b9",
   "metadata": {
    "id": "ig-KVgkCDCKD"
   },
   "source": [
    "# Etcetera\n",
    "\n",
    "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d7c017",
   "metadata": {
    "id": "rIHiVP53FnsX"
   },
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9398a",
   "metadata": {
    "id": "wmTXWNUygS5E"
   },
   "source": [
    "# LICENSE\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 Max Woolf\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867cf26",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-2 small for generating replies for online discussion\n",
    "\n",
    "## Task list \n",
    "\n",
    "### Read cleaned_parent (post) and cleaned_reply (reply) columns from final_cleaned_8330.xlsx file\n",
    "\n",
    "### Split into fine-tune training set and validation set by 80% / 20\n",
    "\n",
    "### Train GPT-2 small using Hugging Face\n",
    "\n",
    "### After training, the fine-tuned model is used to batch inference all 8330 posts to generate replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03c7d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.11/site-packages (4.48.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: accelerate in /opt/homebrew/lib/python3.11/site-packages (1.2.1)\n",
      "Requirement already satisfied: openpyxl in /opt/homebrew/lib/python3.11/site-packages (3.1.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/liuzifeng/Library/Python/3.11/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.8.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from datasets) (2.1.2)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.3)\n",
      "Requirement already satisfied: psutil in /Users/liuzifeng/Library/Python/3.11/lib/python/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/homebrew/lib/python3.11/site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: et-xmlfile in /opt/homebrew/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/liuzifeng/Library/Python/3.11/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/liuzifeng/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-20.0.0-cp311-cp311-macosx_12_0_arm64.whl (30.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.9/30.9 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, tqdm, requests, pyarrow, dill, multiprocess, datasets\n",
      "\u001b[2K  Attempting uninstall: tqdm\n",
      "\u001b[2K    Found existing installation: tqdm 4.66.1\n",
      "\u001b[2K    Uninstalling tqdm-4.66.1:\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.66.1\n",
      "\u001b[2K  Attempting uninstall: requests\n",
      "\u001b[2K    Found existing installation: requests 2.31.0\n",
      "\u001b[2K    Uninstalling requests-2.31.0:\n",
      "\u001b[2K      Successfully uninstalled requests-2.31.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [datasets]6/7\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-3.6.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-20.0.0 requests-2.32.3 tqdm-4.67.1 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets accelerate openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37cd470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9cc7f9",
   "metadata": {},
   "source": [
    "## Step 1 Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48a8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6636/6636 [00:04<00:00, 1408.12 examples/s]\n",
      "Map: 100%|██████████| 1660/1660 [00:01<00:00, 1378.95 examples/s]\n",
      "/opt/homebrew/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/kr/1ch_8lcx0zz7sztc44474v0r0000gn/T/ipykernel_90834/1149708173.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='6636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/6636 03:07 < 346:12:29, 0.01 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read and clean the data\n",
    "df = pd.read_excel(\"final_cleaned_8330.xlsx\")\n",
    "df = df[[\"cleaned_parent\", \"cleaned_reply\"]].dropna()\n",
    "df = df.rename(columns={\"cleaned_parent\": \"post\", \"cleaned_reply\": \"reply\"})\n",
    "\n",
    "# Construct prompt-style training text (core formatting)\n",
    "df[\"text\"] = df.apply(\n",
    "    lambda row: f\"<|startoftext|>Assume you are a teacher or student in an online course forum. Please reply to this post: <Post> {row['post']} <Reply> {row['reply']}<|endoftext|>\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Split into training and validation sets (80% / 20%)\n",
    "train_texts, eval_texts = train_test_split(df[\"text\"], test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts.tolist()})\n",
    "eval_dataset = Dataset.from_dict({\"text\": eval_texts.tolist()})\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"validation\": eval_dataset})\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({\n",
    "    \"bos_token\": \"<|startoftext|>\",\n",
    "    \"eos_token\": \"<|endoftext|>\",\n",
    "    \"pad_token\": \"<|pad|>\"\n",
    "})\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_post_reply_prompt_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=False  # Set to True if using GPU with fp16 support\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_path = \"./gpt2_post_reply_prompt_finetuned\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b254c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = \"./gpt2_post_reply_prompt_finetuned\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Read all post data\n",
    "df = pd.read_excel(\"final_cleaned_8330.xlsx\")\n",
    "df = df[[\"cleaned_parent\", \"cleaned_reply\"]].dropna()\n",
    "df = df.rename(columns={\"cleaned_parent\": \"post\", \"cleaned_reply\": \"true_reply\"})\n",
    "\n",
    "# Construct prompt and generate replies\n",
    "generated_replies = []\n",
    "\n",
    "for post in tqdm(df[\"post\"], desc=\"Generating replies\"):\n",
    "    prompt = f\"<|startoftext|>Assume you are a teacher or student in an online course forum. Please reply to this post: <Post> {post} <Reply>\"\n",
    "    output = generator(prompt, max_new_tokens=80, temperature=0.7)[0][\"generated_text\"]\n",
    "    \n",
    "    # Extract reply segment (starting after <Reply>)\n",
    "    reply = output.split(\"<Reply>\")[-1].split(\"<|endoftext|>\")[0].strip()\n",
    "    generated_replies.append(reply)\n",
    "\n",
    "# Add result column and save\n",
    "df[\"generated_reply\"] = generated_replies\n",
    "df.to_excel(\"gpt2_generated_replies.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658574c8",
   "metadata": {},
   "source": [
    "## Fine-tuning LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2828f9",
   "metadata": {},
   "source": [
    "## Llama fintune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Step 1=========\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig, set_seed\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ========= Step 2=========\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "output_dir = \"./finetuned_llama3-8b\"\n",
    "max_length = 128\n",
    "batch_size = 4\n",
    "gradient_accumulation = 4  # → effective batch size = 16\n",
    "num_epochs = 3\n",
    "seed = 42\n",
    "\n",
    "# ========= Step 3 =========\n",
    "df = pd.read_excel(\"final_cleaned_8330.xlsx\")[[\"cleaned_parent\", \"cleaned_reply\"]].dropna()\n",
    "df = df.rename(columns={\"cleaned_parent\": \"post\", \"cleaned_reply\": \"reply\"})\n",
    "\n",
    "df[\"text\"] = df.apply(\n",
    "    lambda row: f\"<|startoftext|>Assume you are a teacher or student in an online course forum. Please reply to this post: <Post> {row['post']} <Reply> {row['reply']}<|endoftext|>\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "train_texts, eval_texts = train_test_split(df[\"text\"], test_size=0.2, random_state=seed)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"text\": train_texts.tolist()}),\n",
    "    \"validation\": Dataset.from_dict({\"text\": eval_texts.tolist()})\n",
    "})\n",
    "\n",
    "# ========= Step 4: Tokenizer =========\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# ========= Step 5: LoRA =========\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ========= Step 6: =========\n",
    "set_seed(seed)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation,\n",
    "    num_train_epochs=num_epochs,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ========= Step 7=========\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa73576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========= Step 1=========\n",
    "model_path = \"./finetuned_llama3-8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# ========= Step 2 =========\n",
    "df = pd.read_excel(\"final_cleaned_8330.xlsx\")[[\"cleaned_parent\", \"cleaned_reply\"]].dropna()\n",
    "df = df.rename(columns={\"cleaned_parent\": \"post\", \"cleaned_reply\": \"true_reply\"})\n",
    "\n",
    "generated_replies = []\n",
    "for post in tqdm(df[\"post\"], desc=\"Generating...\"):\n",
    "    prompt = f\"<|startoftext|>Assume you are a teacher or student in an online course forum. Please reply to this post: <Post> {post} <Reply>\"\n",
    "    output = generator(prompt, max_new_tokens=80, temperature=0.7)[0][\"generated_text\"]\n",
    "    reply = output.split(\"<Reply>\")[-1].split(\"<|endoftext|>\")[0].strip()\n",
    "    generated_replies.append(reply)\n",
    "\n",
    "df[\"generated_reply\"] = generated_replies\n",
    "df.to_excel(\"llama3_generated_replies.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
